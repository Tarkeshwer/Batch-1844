{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4beb74ac",
   "metadata": {},
   "source": [
    "# Assignment - 4 Web_Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842eb3bd",
   "metadata": {},
   "source": [
    "‚Ä¢ Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of\n",
    "your choice.  \n",
    "‚Ä¢ You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get\n",
    "information about selenium Exceptions.   \n",
    "  You may visit following links:\n",
    "1. https://selenium-python.readthedocs.io/api.html\n",
    "2. https://www.guru99.com/exception-handling-selenium.html\n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-elementexception/38023345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cafb5",
   "metadata": {},
   "source": [
    "Q1. Scrape the details of most viewed videos on YouTube from Wikipedia.  \n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos  \n",
    "You need to find following details:  \n",
    "A) Rank  \n",
    "B) Name  \n",
    "C) Artist  \n",
    "D) Upload date  \n",
    "E) Views  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1668be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.4.3)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.21.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n"
     ]
    }
   ],
   "source": [
    "#install selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e027b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import time\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "import re\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa68a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "\n",
    "# Getting the webpage of url \n",
    "\n",
    "url=('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c404a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list for storing data after scraping\n",
    "\n",
    "Rank = []\n",
    "Name = []\n",
    "Artist = []\n",
    "UploadDate = []\n",
    "Views = []\n",
    "\n",
    "\n",
    "# Scraping Rank of the videos\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "    \n",
    "#scrapping rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[2]\"):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"_\")\n",
    "    \n",
    "    \n",
    "#scrapping Artist\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//*[@id=\"mw-content-text\"]/div[1]/table[2]/tbody/tr/td[3]'):\n",
    "        Artist.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Artist.append(\"_\")\n",
    "    \n",
    "    \n",
    "#Scrapping Date\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,' //table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]'):\n",
    "        UploadDate.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    UploadDate.append('-')\n",
    "# Scraping Views of videos\n",
    "\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='wikitable sortable jquery-tablesorter'][1]/tbody/tr/td[4]\"):\n",
    "        Views.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Views.append(\"_\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f43497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataFrame from scraped data\n",
    "wikipedia = pd.DataFrame({})\n",
    "wikipedia['Rank']=Rank\n",
    "wikipedia['Name']=Name\n",
    "wikipedia['Artist']=Artist\n",
    "wikipedia['Upload Date']=UploadDate\n",
    "wikipedia['Views']=Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd0b1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload Date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[3]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[6]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>7.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[12]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Shape of You\"[13]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"See You Again\"[15]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"Bath Song\"[20]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[21]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Uptown Funk\"[22]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Learning Colors ‚Äì Colorful Eggs on a Farm\"[23]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Gangnam Style\"[24]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Masha and the Bear ‚Äì Recipe for Disaster\"[29]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Wheels on the Bus\"[30]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>4.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[31]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sugar\"[32]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[33]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[34]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Sorry\"[35]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Axel F\"[36]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Thinking Out Loud\"[37]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[38]</td>\n",
       "      <td>Cocomelon ‚Äì Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Dark Horse\"[39]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Faded\"[40]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Girls Like You\"[41]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Let Her Go\"[42]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Bailando\"[43]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Lean On\"[44]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Perfect\"[45]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[46]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Shake It Off\"[47]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>August 18, 2014</td>\n",
       "      <td>3.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[48]</td>\n",
       "      <td>Kiddiestv Hindi ‚Äì Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[3]   \n",
       "1    2.                                   \"Despacito\"[6]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[12]   \n",
       "3    4.                               \"Shape of You\"[13]   \n",
       "4    5.                              \"See You Again\"[15]   \n",
       "5    6.                                  \"Bath Song\"[20]   \n",
       "6    7.                \"Phonics Song with Two Words\"[21]   \n",
       "7    8.                                \"Uptown Funk\"[22]   \n",
       "8    9.  \"Learning Colors ‚Äì Colorful Eggs on a Farm\"[23]   \n",
       "9   10.                              \"Gangnam Style\"[24]   \n",
       "10  11.   \"Masha and the Bear ‚Äì Recipe for Disaster\"[29]   \n",
       "11  12.                          \"Wheels on the Bus\"[30]   \n",
       "12  13.                             \"Dame Tu Cosita\"[31]   \n",
       "13  14.                                      \"Sugar\"[32]   \n",
       "14  15.                                       \"Roar\"[33]   \n",
       "15  16.                             \"Counting Stars\"[34]   \n",
       "16  17.                                      \"Sorry\"[35]   \n",
       "17  18.                                     \"Axel F\"[36]   \n",
       "18  18.                          \"Thinking Out Loud\"[37]   \n",
       "19  20.                        \"Baa Baa Black Sheep\"[38]   \n",
       "20  21.                                 \"Dark Horse\"[39]   \n",
       "21  22.                                      \"Faded\"[40]   \n",
       "22  23.                             \"Girls Like You\"[41]   \n",
       "23  24.                                 \"Let Her Go\"[42]   \n",
       "24  25.                                   \"Bailando\"[43]   \n",
       "25  26.                                    \"Lean On\"[44]   \n",
       "26  27.                                    \"Perfect\"[45]   \n",
       "27  28.           \"Waka Waka (This Time for Africa)\"[46]   \n",
       "28  29.                               \"Shake It Off\"[47]   \n",
       "29  30.          \"Humpty the train on a fruits ride\"[48]   \n",
       "\n",
       "                                           Artist        Upload Date  Views  \n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016  11.45  \n",
       "1                                      Luis Fonsi   January 12, 2017   7.98  \n",
       "2                                     LooLoo Kids    October 8, 2016   6.48  \n",
       "3                                      Ed Sheeran   January 30, 2017   5.82  \n",
       "4                                     Wiz Khalifa      April 6, 2015   5.66  \n",
       "5                      Cocomelon ‚Äì Nursery Rhymes        May 2, 2018   5.64  \n",
       "6                                       ChuChu TV      March 6, 2014   4.92  \n",
       "7                                     Mark Ronson  November 19, 2014   4.71  \n",
       "8                                     Miroshka TV  February 27, 2018   4.67  \n",
       "9                                             Psy      July 15, 2012   4.56  \n",
       "10                                     Get Movies   January 31, 2012   4.51  \n",
       "11                     Cocomelon ‚Äì Nursery Rhymes       May 24, 2018   4.44  \n",
       "12                                      El Chombo      April 5, 2018   4.08  \n",
       "13                                       Maroon 5   January 14, 2015   3.76  \n",
       "14                                     Katy Perry  September 5, 2013   3.66  \n",
       "15                                    OneRepublic       May 31, 2013   3.66  \n",
       "16                                  Justin Bieber   October 22, 2015   3.59  \n",
       "17                                     Crazy Frog      June 16, 2009   3.54  \n",
       "18                                     Ed Sheeran    October 7, 2014   3.50  \n",
       "19                     Cocomelon ‚Äì Nursery Rhymes      June 25, 2018   3.39  \n",
       "20                                     Katy Perry  February 20, 2014   3.35  \n",
       "21                                    Alan Walker   December 3, 2015   3.35  \n",
       "22                                       Maroon 5       May 31, 2018   3.33  \n",
       "23                                      Passenger      July 25, 2012   3.31  \n",
       "24                               Enrique Iglesias     April 11, 2014   3.28  \n",
       "25                                    Major Lazer     March 22, 2015   3.27  \n",
       "26                                     Ed Sheeran   November 9, 2017   3.27  \n",
       "27                                        Shakira       June 4, 2010   3.26  \n",
       "28                                   Taylor Swift    August 18, 2014   3.21  \n",
       "29  Kiddiestv Hindi ‚Äì Nursery Rhymes & Kids Songs   January 26, 2018   3.18  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a76a5318",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b7f2e",
   "metadata": {},
   "source": [
    "Q2. Scrape the details team India‚Äôs international fixtures from bcci.tv.  \n",
    "Url = https://www.bcci.tv/.  \n",
    "You need to find following details:  \n",
    "A) Match title (I.e. 1st ODI)  \n",
    "B) Series  \n",
    "C) Place  \n",
    "D) Date  \n",
    "E) Time  \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4248ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "\n",
    "# Getting the webpage of mentioned url \n",
    "url=('https://www.bcci.tv/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19a7ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clickin on International tab\n",
    "International = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a') # click button\n",
    "International.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff0ed072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Empty List\n",
    "Name=[]\n",
    "Series=[]\n",
    "Place=[]\n",
    "Date=[]\n",
    "Time=[]\n",
    "#Scrapping Name\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//span[@class=\"matchOrderText ng-binding ng-scope\"]'):\n",
    "        Name.append(i.text.replace('-',\"\"))\n",
    "except NoSuchElementException:\n",
    "    Name.append('-')\n",
    "\n",
    "#Scrapping Series\n",
    "try:\n",
    "    \n",
    "    for i in driver.find_elements(By.XPATH,'//span[@class=\"ng-binding\"]'):\n",
    "        Series.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Series.append('-')\n",
    "#scrapping Place\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//span[@class=\"ng-binding ng-scope\"]'):\n",
    "        Place.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Place.append('-')\n",
    "#Scrapping Date\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//h5[@class=\"ng-binding\"]'):\n",
    "        Date.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Date.append('-')\n",
    "#Scrapping Time\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//h5[@class=\"text-right ng-binding\"]'):\n",
    "        Time.append(i.text.replace('IST',\"\"))\n",
    "except NoSuchElementException:\n",
    "    Time.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d33a818f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6 6 6 6\n"
     ]
    }
   ],
   "source": [
    "print(len(Name),len(Series),len(Place),len(Date),len(Time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de9931b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "\n",
    "Fixtures=pd.DataFrame({})\n",
    "Fixtures['Title']=Series\n",
    "Fixtures['Name']=Name\n",
    "Fixtures['Place']=Place\n",
    "Fixtures['Date']=Date\n",
    "Fixtures['Time']=Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a7c6a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Name</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23</td>\n",
       "      <td>3rd ODI</td>\n",
       "      <td>Arun Jaitley Stadium,</td>\n",
       "      <td>11 OCT 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>1st T20I</td>\n",
       "      <td>Melbourne Cricket Ground,</td>\n",
       "      <td>23 OCT 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>2nd T20I</td>\n",
       "      <td>Sydney Cricket Ground,</td>\n",
       "      <td>27 OCT 2022</td>\n",
       "      <td>12:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>3rd T20I</td>\n",
       "      <td>WACA Ground,</td>\n",
       "      <td>30 OCT 2022</td>\n",
       "      <td>4:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>4th T20I</td>\n",
       "      <td>Adelaide Oval,</td>\n",
       "      <td>2 NOV 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ICC MENS T20 WORLD CUP 2022</td>\n",
       "      <td>5th T20I</td>\n",
       "      <td>Melbourne Cricket Ground,</td>\n",
       "      <td>6 NOV 2022</td>\n",
       "      <td>1:30 PM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Title       Name  \\\n",
       "0  SOUTH AFRICA TOUR OF INDIA ODI SERIES 2022-23   3rd ODI    \n",
       "1                    ICC MENS T20 WORLD CUP 2022  1st T20I    \n",
       "2                    ICC MENS T20 WORLD CUP 2022  2nd T20I    \n",
       "3                    ICC MENS T20 WORLD CUP 2022  3rd T20I    \n",
       "4                    ICC MENS T20 WORLD CUP 2022  4th T20I    \n",
       "5                    ICC MENS T20 WORLD CUP 2022  5th T20I    \n",
       "\n",
       "                       Place         Date       Time  \n",
       "0      Arun Jaitley Stadium,  11 OCT 2022   1:30 PM   \n",
       "1  Melbourne Cricket Ground,  23 OCT 2022   1:30 PM   \n",
       "2     Sydney Cricket Ground,  27 OCT 2022  12:30 PM   \n",
       "3               WACA Ground,  30 OCT 2022   4:30 PM   \n",
       "4             Adelaide Oval,   2 NOV 2022   1:30 PM   \n",
       "5  Melbourne Cricket Ground,   6 NOV 2022   1:30 PM   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "895102cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f251e",
   "metadata": {},
   "source": [
    "Q3. Scrape the details of selenium exception from guru99.com.  \n",
    "Url = https://www.guru99.com/  \n",
    "You need to find following details:  \n",
    "A) Name  \n",
    "B) Description  \n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b86a99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "\n",
    "# get webpage\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8695776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click Selenium Button\n",
    "Selenium = driver.find_element(By.XPATH,'//div[@class = \"wp-block-kadence-column inner-column-2 kadence-column_f38d98-3a\"]') # click button\n",
    "try:\n",
    "    Selenium.click()\n",
    "except ElementNotInteractableException:#handling element not clickable exception\n",
    "    driver.get(Selenium.get_attribute('href'))\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e09dc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click selenium exception handling  Button\n",
    "exception_handling = driver.find_element(By.XPATH,\"//a[@title='Selenium Exception Handling (Common Exceptions List)']\")\n",
    "try:\n",
    "    exception_handling.click()\n",
    "except ElementNotInteractableException:  \n",
    "#if the above code doesn't work/is not clickable then, the below code will handle it\n",
    "    driver.get(exception_handling.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "860d4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "name=[]\n",
    "desc=[]\n",
    "#Scrapping Name\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[1]/div/div/div/main/div/article/div/div/p[1]/strong\"):\n",
    "    name.append(i.text)\n",
    "for i in driver.find_elements(By.XPATH,\"/html/body/div[1]/div/div/div/main/div/article/div/div/p[1]\"):\n",
    "    desc.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "addd711b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exception name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. ElementNotVisibleException:</td>\n",
       "      <td>1. ElementNotVisibleException: This type of Se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Exception name  \\\n",
       "0  1. ElementNotVisibleException:   \n",
       "\n",
       "                                         Description  \n",
       "0  1. ElementNotVisibleException: This type of Se...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selenium_Exception=pd.DataFrame()\n",
    "Selenium_Exception['Exception name']=name\n",
    "Selenium_Exception['Description']=desc\n",
    "Selenium_Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "724b7a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cffcc",
   "metadata": {},
   "source": [
    "Q4. Scrape the details of State-wise GDP of India from statisticstime.com.  \n",
    "Url = http://statisticstimes.com/  \n",
    "You have to find following details:  \n",
    "A) Rank  \n",
    "B) State  \n",
    "C) GSDP(18-19)  \n",
    "D) GSDP(17-18)  \n",
    "E) Share(2017)  \n",
    "F) GDP($ billion)  \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "206ad796",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "driver.get('https://www.statisticstimes.com')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eaff954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element(By.XPATH,'/html/body/div[1]/div/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15a99c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on Economy button\n",
    "driver.find_element(By.XPATH,\"//div[@class='navbar']/div[2]/button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "274b936a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WebElement.click of <selenium.webdriver.remote.webelement.WebElement (session=\"b2ef4d441b5423747e9d310568b23294\", element=\"c8701134-9eb6-4876-a075-15d24619c6b6\")>>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clicking on India\n",
    "driver.find_element(By.XPATH,\"//div[@class='dropdown-content']/a[3]\").click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f80f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on GDP of Indian Economy\n",
    "GDP = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a').click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f6d740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP1 = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDPbillion = []\n",
    "# Scraping Rank\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[1]\"):\n",
    "        Rank.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Rank.append(\"_\")\n",
    "# Scraping State\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[2]\"):\n",
    "        State.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    State.append(\"_\")\n",
    "    \n",
    "# Scraping GSDP at current price (19-20)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[3]\"):\n",
    "        GSDP1.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP1.append(\"_\")\n",
    "    # Scraping GSDP at current price (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[4]\"):\n",
    "        GSDP2.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GSDP2.append(\"_\")\n",
    "# Scraping Share (18-19)\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[5]\"):\n",
    "        Share.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Share.append(\"_\")\n",
    "# Scraping GDP $ billion\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,\"//table[@class='display dataTable']/tbody/tr/td[6]\"):\n",
    "        GDPbillion.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    GDPbillion.append(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b57cb3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>STATE</th>\n",
       "      <th>GSDP_19-20</th>\n",
       "      <th>GSDP_18-19</th>\n",
       "      <th>SHARE</th>\n",
       "      <th>GDPbillion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>-</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>29</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,391</td>\n",
       "      <td>25,141</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>17,060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>-</td>\n",
       "      <td>24,534</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>-</td>\n",
       "      <td>22,488</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>24,424</td>\n",
       "      <td>20,947</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>17,797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RANK                      STATE GSDP_19-20 GSDP_18-19   SHARE GDPbillion\n",
       "0     1                Maharashtra          -  2,632,792  13.94%    399.921\n",
       "1     2                 Tamil Nadu  1,845,853  1,630,208   8.63%    247.629\n",
       "2     3              Uttar Pradesh  1,687,818  1,584,764   8.39%    240.726\n",
       "3     4                    Gujarat          -  1,502,899   7.96%    228.290\n",
       "4     5                  Karnataka  1,631,977  1,493,127   7.91%    226.806\n",
       "..  ...                        ...        ...        ...     ...        ...\n",
       "61   29                     Sikkim     28,391     25,141   0.15%     17,060\n",
       "62   30                   Nagaland          -     24,534   0.15%          -\n",
       "63   31          Arunachal Pradesh          -     22,488   0.13%          -\n",
       "64   32                    Mizoram     24,424     20,947   0.13%     17,797\n",
       "65   33  Andaman & Nicobar Islands          -          -       -          -\n",
       "\n",
       "[66 rows x 6 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDIA=pd.DataFrame()\n",
    "INDIA['RANK']=Rank\n",
    "INDIA['STATE']=State\n",
    "INDIA['GSDP_19-20']=GSDP1\n",
    "INDIA['GSDP_18-19']=GSDP2\n",
    "INDIA['SHARE']=Share\n",
    "INDIA['GDPbillion']=GDPbillion\n",
    "INDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74eecd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDIA.to_csv(r\"E:\\Flip Robo\\Data\\GDP.csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a6fd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96376371",
   "metadata": {},
   "source": [
    "Q5. Scrape the details of trending repositories on Github.com.  \n",
    "Url = https://github.com/  \n",
    "You have to find the following details:  \n",
    "A) Repository title  \n",
    "B) Repository description  \n",
    "C) Contributors count  \n",
    "D) Language used  \n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "267e9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "url=('https://github.com/')\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06ed8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on explore\n",
    "\n",
    "explore = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[2]/div/nav/ul/li[3]')\n",
    "try:\n",
    "    explore.click()\n",
    "    time.sleep(5)\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(explore.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83e529c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click on Trending\n",
    "\n",
    "trending = driver.find_element(By.XPATH,'//*[@href=\"/trending\"]')\n",
    "try:\n",
    "    driver.get(trending.get_attribute('href'))\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(trending.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2d4b75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list:\n",
    "URLs = []\n",
    "repository_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []\n",
    "\n",
    "#Fetching urls for each repository\n",
    "repository = driver.find_elements(By.XPATH,\"//h1[@class = 'h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    URLs.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e15e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Repository Title data\n",
    "title = driver.find_elements(By.XPATH,\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    repository_title.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6aac88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from all repository page\n",
    "for i in URLs:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "        # Scraping Repository Description data\n",
    "    try:\n",
    "        desc = driver.find_element(By.XPATH,\"//p[@class='f4 my-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    # Scraping Contributors Count data\n",
    "    try:\n",
    "        contributor = driver.find_element(By.XPATH,\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(contributor.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "        \n",
    "        \n",
    "     # Scraping Languages used data\n",
    "    lang=[]\n",
    "    try:\n",
    "        for i in driver.find_elements(By.XPATH,'//span[@class=\"color-fg-default text-bold mr-1\"]'):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        lang.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb4139c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25 25 25\n"
     ]
    }
   ],
   "source": [
    "print(len(repository_title),len(Description),len(Contributors),len(Language))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b99785ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AUTOMATIC1111 / stable-diffusion-webui</td>\n",
       "      <td>Stable Diffusion web UI</td>\n",
       "      <td>109</td>\n",
       "      <td>[Python, JavaScript, CSS, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AykutSarac / jsoncrack.com</td>\n",
       "      <td>üîÆ Seamlessly visualize your JSON data instantl...</td>\n",
       "      <td>16</td>\n",
       "      <td>[TypeScript, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type-challenges / type-challenges</td>\n",
       "      <td>Collection of TypeScript type challenges with ...</td>\n",
       "      <td>170</td>\n",
       "      <td>[TypeScript, HTML, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>inancgumus / learngo</td>\n",
       "      <td>‚ù§Ô∏è 1000+ Hand-Crafted Go Examples, Exercises, ...</td>\n",
       "      <td>27</td>\n",
       "      <td>[Go, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apache / skywalking</td>\n",
       "      <td>APM, Application Performance Monitoring System</td>\n",
       "      <td>413</td>\n",
       "      <td>[Java, Shell, FreeMarker, ANTLR, Python, Lua, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TencentARC / GFPGAN</td>\n",
       "      <td>GFPGAN aims at developing Practical Algorithms...</td>\n",
       "      <td>11</td>\n",
       "      <td>[Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iptv-org / iptv</td>\n",
       "      <td>Collection of publicly available IPTV channels...</td>\n",
       "      <td>192</td>\n",
       "      <td>[JavaScript, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dail8859 / NotepadNext</td>\n",
       "      <td>A cross-platform, reimplementation of Notepad++</td>\n",
       "      <td>13</td>\n",
       "      <td>[C++, HTML, C, Lua, NSIS, Objective-C++, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wagtail / wagtail</td>\n",
       "      <td>A Django content management system focused on ...</td>\n",
       "      <td>578</td>\n",
       "      <td>[Python, JavaScript, HTML, TypeScript, SCSS, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apache / doris</td>\n",
       "      <td>Apache Doris is an easy-to-use, high performan...</td>\n",
       "      <td>392</td>\n",
       "      <td>[Java, C++, Groovy, C, Python, Thrift, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>heejkoo / Awesome-Diffusion-Models</td>\n",
       "      <td>A collection of resources and papers on Diffus...</td>\n",
       "      <td>15</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>skydoves / android-developer-roadmap</td>\n",
       "      <td>üó∫ The 2022 Android Developer Roadmap suggests ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[Kotlin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>vbenjs / vue-vben-admin</td>\n",
       "      <td>A modern vue admin. It is based on Vue3, vite ...</td>\n",
       "      <td>151</td>\n",
       "      <td>[Vue, TypeScript, Less, JavaScript, CSS, HTML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>CompVis / stable-diffusion</td>\n",
       "      <td>A latent text-to-image diffusion model</td>\n",
       "      <td>7</td>\n",
       "      <td>[Jupyter Notebook, Python, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>juliocesarfort / public-pentesting-reports</td>\n",
       "      <td>Curated list of public penetration test report...</td>\n",
       "      <td>34</td>\n",
       "      <td>[CSS, C, JavaScript, Makefile, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PaddlePaddle / PaddleOCR</td>\n",
       "      <td>Awesome multilingual OCR toolkits based on Pad...</td>\n",
       "      <td>105</td>\n",
       "      <td>[Python, C++, Shell, Java, CMake, Makefile, Ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>zadam / trilium</td>\n",
       "      <td>Build your personal knowledge base with Triliu...</td>\n",
       "      <td>70</td>\n",
       "      <td>[JavaScript, CSS, EJS, Shell, Dockerfile, Batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>remoteintech / remote-jobs</td>\n",
       "      <td>A list of semi to fully remote-friendly compan...</td>\n",
       "      <td>688</td>\n",
       "      <td>[JavaScript, HTML, CSS, Shell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CyC2018 / CS-Notes</td>\n",
       "      <td>üìö ÊäÄÊúØÈù¢ËØïÂøÖÂ§áÂü∫Á°ÄÁü•ËØÜ„ÄÅLeetcode„ÄÅËÆ°ÁÆóÊú∫Êìç‰ΩúÁ≥ªÁªü„ÄÅËÆ°ÁÆóÊú∫ÁΩëÁªú„ÄÅÁ≥ªÁªüËÆæËÆ°</td>\n",
       "      <td>231</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xuxueli / xxl-job</td>\n",
       "      <td>A distributed task scheduling framework.ÔºàÂàÜÂ∏ÉÂºè‰ªªÂä°...</td>\n",
       "      <td>58</td>\n",
       "      <td>[Java, Dockerfile]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>apache / dolphinscheduler</td>\n",
       "      <td>Apache DolphinScheduler is a distributed and e...</td>\n",
       "      <td>405</td>\n",
       "      <td>[Java, TypeScript, Python, PLpgSQL, SCSS, Shel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>halo-dev / halo</td>\n",
       "      <td>‚úç ‰∏ÄÊ¨æÁé∞‰ª£ÂåñÁöÑÂºÄÊ∫êÂçöÂÆ¢ / CMS Á≥ªÁªü„ÄÇ</td>\n",
       "      <td>74</td>\n",
       "      <td>[Java, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>apache / echarts</td>\n",
       "      <td>Apache ECharts is a powerful, interactive char...</td>\n",
       "      <td>177</td>\n",
       "      <td>[TypeScript, JavaScript, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OpenIMSDK / Open-IM-Server</td>\n",
       "      <td>Âç≥Êó∂ÈÄöËÆØIM</td>\n",
       "      <td>27</td>\n",
       "      <td>[Go, Shell, Other]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>yangzongzhuan / RuoYi-Vue3</td>\n",
       "      <td>üéâ (RuoYi)ÂÆòÊñπ‰ªìÂ∫ì Âü∫‰∫éSpringBootÔºåSpring SecurityÔºåJWT...</td>\n",
       "      <td>9</td>\n",
       "      <td>[Vue, JavaScript, HTML, SCSS, Batchfile]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Repository title  \\\n",
       "0       AUTOMATIC1111 / stable-diffusion-webui   \n",
       "1                   AykutSarac / jsoncrack.com   \n",
       "2            type-challenges / type-challenges   \n",
       "3                         inancgumus / learngo   \n",
       "4                          apache / skywalking   \n",
       "5                          TencentARC / GFPGAN   \n",
       "6                              iptv-org / iptv   \n",
       "7                       dail8859 / NotepadNext   \n",
       "8                            wagtail / wagtail   \n",
       "9                               apache / doris   \n",
       "10          heejkoo / Awesome-Diffusion-Models   \n",
       "11        skydoves / android-developer-roadmap   \n",
       "12                     vbenjs / vue-vben-admin   \n",
       "13                  CompVis / stable-diffusion   \n",
       "14  juliocesarfort / public-pentesting-reports   \n",
       "15                    PaddlePaddle / PaddleOCR   \n",
       "16                             zadam / trilium   \n",
       "17                  remoteintech / remote-jobs   \n",
       "18                          CyC2018 / CS-Notes   \n",
       "19                           xuxueli / xxl-job   \n",
       "20                   apache / dolphinscheduler   \n",
       "21                             halo-dev / halo   \n",
       "22                            apache / echarts   \n",
       "23                  OpenIMSDK / Open-IM-Server   \n",
       "24                  yangzongzhuan / RuoYi-Vue3   \n",
       "\n",
       "                               Repository description Contributors count  \\\n",
       "0                             Stable Diffusion web UI                109   \n",
       "1   üîÆ Seamlessly visualize your JSON data instantl...                 16   \n",
       "2   Collection of TypeScript type challenges with ...                170   \n",
       "3   ‚ù§Ô∏è 1000+ Hand-Crafted Go Examples, Exercises, ...                 27   \n",
       "4      APM, Application Performance Monitoring System                413   \n",
       "5   GFPGAN aims at developing Practical Algorithms...                 11   \n",
       "6   Collection of publicly available IPTV channels...                192   \n",
       "7     A cross-platform, reimplementation of Notepad++                 13   \n",
       "8   A Django content management system focused on ...                578   \n",
       "9   Apache Doris is an easy-to-use, high performan...                392   \n",
       "10  A collection of resources and papers on Diffus...                 15   \n",
       "11  üó∫ The 2022 Android Developer Roadmap suggests ...                 15   \n",
       "12  A modern vue admin. It is based on Vue3, vite ...                151   \n",
       "13             A latent text-to-image diffusion model                  7   \n",
       "14  Curated list of public penetration test report...                 34   \n",
       "15  Awesome multilingual OCR toolkits based on Pad...                105   \n",
       "16  Build your personal knowledge base with Triliu...                 70   \n",
       "17  A list of semi to fully remote-friendly compan...                688   \n",
       "18           üìö ÊäÄÊúØÈù¢ËØïÂøÖÂ§áÂü∫Á°ÄÁü•ËØÜ„ÄÅLeetcode„ÄÅËÆ°ÁÆóÊú∫Êìç‰ΩúÁ≥ªÁªü„ÄÅËÆ°ÁÆóÊú∫ÁΩëÁªú„ÄÅÁ≥ªÁªüËÆæËÆ°                231   \n",
       "19  A distributed task scheduling framework.ÔºàÂàÜÂ∏ÉÂºè‰ªªÂä°...                 58   \n",
       "20  Apache DolphinScheduler is a distributed and e...                405   \n",
       "21                             ‚úç ‰∏ÄÊ¨æÁé∞‰ª£ÂåñÁöÑÂºÄÊ∫êÂçöÂÆ¢ / CMS Á≥ªÁªü„ÄÇ                 74   \n",
       "22  Apache ECharts is a powerful, interactive char...                177   \n",
       "23                                             Âç≥Êó∂ÈÄöËÆØIM                 27   \n",
       "24  üéâ (RuoYi)ÂÆòÊñπ‰ªìÂ∫ì Âü∫‰∫éSpringBootÔºåSpring SecurityÔºåJWT...                  9   \n",
       "\n",
       "                                        Language used  \n",
       "0                    [Python, JavaScript, CSS, Other]  \n",
       "1                                 [TypeScript, Other]  \n",
       "2                           [TypeScript, HTML, Other]  \n",
       "3                                         [Go, Other]  \n",
       "4   [Java, Shell, FreeMarker, ANTLR, Python, Lua, ...  \n",
       "5                                            [Python]  \n",
       "6                                 [JavaScript, Shell]  \n",
       "7     [C++, HTML, C, Lua, NSIS, Objective-C++, Other]  \n",
       "8   [Python, JavaScript, HTML, TypeScript, SCSS, S...  \n",
       "9       [Java, C++, Groovy, C, Python, Thrift, Other]  \n",
       "10                                                 []  \n",
       "11                                           [Kotlin]  \n",
       "12     [Vue, TypeScript, Less, JavaScript, CSS, HTML]  \n",
       "13                  [Jupyter Notebook, Python, Shell]  \n",
       "14              [CSS, C, JavaScript, Makefile, Shell]  \n",
       "15  [Python, C++, Shell, Java, CMake, Makefile, Ot...  \n",
       "16  [JavaScript, CSS, EJS, Shell, Dockerfile, Batc...  \n",
       "17                     [JavaScript, HTML, CSS, Shell]  \n",
       "18                                                 []  \n",
       "19                                 [Java, Dockerfile]  \n",
       "20  [Java, TypeScript, Python, PLpgSQL, SCSS, Shel...  \n",
       "21                                      [Java, Other]  \n",
       "22                    [TypeScript, JavaScript, Other]  \n",
       "23                                 [Go, Shell, Other]  \n",
       "24           [Vue, JavaScript, HTML, SCSS, Batchfile]  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data Frame\n",
    "\n",
    "Github=pd.DataFrame({})\n",
    "Github['Repository title'] = repository_title\n",
    "Github['Repository description'] = Description\n",
    "Github['Contributors count'] = Contributors\n",
    "Github['Language used'] = Language\n",
    "Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38a8ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914683e5",
   "metadata": {},
   "source": [
    "Q6. Scrape the details of top 100 songs on billiboard.com.  \n",
    "Url = https:/www.billboard.com/  \n",
    "You have to find the following details:  \n",
    "A) Song name  \n",
    "B) Artist name  \n",
    "C) Last week rank  \n",
    "D) Peak rank  \n",
    "E) Weeks on board  \n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "26f583c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "driver.maximize_window()\n",
    "# Getting the webpage of mentioned url \n",
    "driver.get('https://www.billboard.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9166175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on charts\n",
    "charts=driver.find_elements(By.XPATH,'//*[@id=\"main-wrapper\"]/header/div[2]/div/nav/ul/li[1]/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bcf8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty lists\n",
    "Song_Name = []\n",
    "Artist_Name = []\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f16d666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping name\n",
    "for i in driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p\"]'):\n",
    "    Song_Name.append(i.text)\n",
    "len(Song_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4001c70a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']\"}\n  (Session info: chrome=106.0.5249.103)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00241ED3+2236115]\n\tOrdinal0 [0x001D92F1+1807089]\n\tOrdinal0 [0x000E66FD+812797]\n\tOrdinal0 [0x001155DF+1005023]\n\tOrdinal0 [0x001157CB+1005515]\n\tOrdinal0 [0x00147632+1209906]\n\tOrdinal0 [0x00131AD4+1120980]\n\tOrdinal0 [0x001459E2+1202658]\n\tOrdinal0 [0x001318A6+1120422]\n\tOrdinal0 [0x0010A73D+960317]\n\tOrdinal0 [0x0010B71F+964383]\n\tGetHandleVerifier [0x004EE7E2+2743074]\n\tGetHandleVerifier [0x004E08D4+2685972]\n\tGetHandleVerifier [0x002D2BAA+532202]\n\tGetHandleVerifier [0x002D1990+527568]\n\tOrdinal0 [0x001E080C+1837068]\n\tOrdinal0 [0x001E4CD8+1854680]\n\tOrdinal0 [0x001E4DC5+1854917]\n\tOrdinal0 [0x001EED64+1895780]\n\tBaseThreadInitThunk [0x7630FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77557B5E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77557B2E+238]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Scrappin Artist name 1 st one\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Artist_Name\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//span[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Remainig Artist Name\u001b[39;00m\n\u001b[0;32m      4\u001b[0m artistTag\u001b[38;5;241m=\u001b[39mdriver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mXPATH,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m//span[@class=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:855\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    852\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    853\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:428\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    426\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    430\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:243\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']\"}\n  (Session info: chrome=106.0.5249.103)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00241ED3+2236115]\n\tOrdinal0 [0x001D92F1+1807089]\n\tOrdinal0 [0x000E66FD+812797]\n\tOrdinal0 [0x001155DF+1005023]\n\tOrdinal0 [0x001157CB+1005515]\n\tOrdinal0 [0x00147632+1209906]\n\tOrdinal0 [0x00131AD4+1120980]\n\tOrdinal0 [0x001459E2+1202658]\n\tOrdinal0 [0x001318A6+1120422]\n\tOrdinal0 [0x0010A73D+960317]\n\tOrdinal0 [0x0010B71F+964383]\n\tGetHandleVerifier [0x004EE7E2+2743074]\n\tGetHandleVerifier [0x004E08D4+2685972]\n\tGetHandleVerifier [0x002D2BAA+532202]\n\tGetHandleVerifier [0x002D1990+527568]\n\tOrdinal0 [0x001E080C+1837068]\n\tOrdinal0 [0x001E4CD8+1854680]\n\tOrdinal0 [0x001E4DC5+1854917]\n\tOrdinal0 [0x001EED64+1895780]\n\tBaseThreadInitThunk [0x7630FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77557B5E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77557B2E+238]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "#Scrappin Artist name 1 st one\n",
    "Artist_Name.append(driver.find_element(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only u-font-size-20@tablet']\").text)\n",
    "#Remainig Artist Name\n",
    "artistTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-no-trucate a-font-primary-s lrv-u-font-size-14@mobile-max u-line-height-normal@mobile-max u-letter-spacing-0021 lrv-u-display-block a-truncate-ellipsis-2line u-max-width-330 u-max-width-230@tablet-only']\")\n",
    "Artist_Name.extend([i.text for i in artistTag])\n",
    "#Scapping Rank\n",
    "rank=[]\n",
    "rankTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-font-primary-bold-l a-font-primary-m@mobile-max u-font-weight-normal@mobile-max lrv-u-padding-tb-050@mobile-max u-font-size-32@tablet']\")\n",
    "rank.extend([i.text for i in rankTag[:3]])\n",
    "#Remaining RAnk\n",
    "Rank=[]\n",
    "RankTag=driver.find_elements(By.XPATH,\"//span[@class='c-label  a-font-primary-m lrv-u-padding-tb-050@mobile-max']\")\n",
    "Rank.extend([i.text for i in RankTag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61abaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing ''\n",
    "for i in Rank:\n",
    "    if i=='':\n",
    "        Rank.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing as per requirement\n",
    "lastweekpos=Rank[0::3]\n",
    "lastweekpos.insert(0,rank[0])\n",
    "peakPos=Rank[1::3]\n",
    "peakPos.insert(0,rank[1])\n",
    "weeksonBoard=Rank[2::3]\n",
    "weeksonBoard.insert(0,rank[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of all\n",
    "len(Song_Name),len(Artist_Name),len(lastweekpos),len(peakPos),len(weeksonBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c095d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe\n",
    "df=pd.DataFrame()\n",
    "df['SongName']=Song_Name[0:100]\n",
    "df['ArtistName']=Artist_Name\n",
    "df['Last Week']=lastweekpos[0:100]\n",
    "df[\"PeekPosition\"]=peakPos[0:100]\n",
    "df['Weeks On board']=weeksonBoard[0:100]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3afe16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2b97f",
   "metadata": {},
   "source": [
    "Q7. Scrape the details of Data science recruiters from naukri.com.  \n",
    "Url = https://www.naukri.com/  \n",
    "You have to find the following details:  \n",
    "A) Name  \n",
    "B) Designation  \n",
    "C) Company  \n",
    "D) Skills they hire for  \n",
    "E) Location  \n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c053f96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "\n",
    "driver.maximize_window()\n",
    "# Getting the webpage of mentioned url \n",
    "url=('https://www.naukri.com/')\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1a9607af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching urls to navigate recruiter page\n",
    "recruiter = driver.find_element(By.XPATH,'//a[@title=\"Search Jobs\"]')\n",
    "page_url = recruiter.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cadb5347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching search button, sending keys and clicking on it\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[6]/div/div/div[1]/div/div/div/input\") \n",
    "search.send_keys(\"Data science \")           \n",
    "btn = driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div/div/div[6]').click()     \n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e1a9c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty list\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []\n",
    "\n",
    "# Scraping Name\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]'):\n",
    "        Name.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Name.append(\"_\")\n",
    "# Scraping Designation\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]'):\n",
    "        Designation.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Designation.append(\"_\")\n",
    "    \n",
    "# Scraping Company\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]'):\n",
    "        Company.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Company.append(\"_\")\n",
    "    # Scraping Skills\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//a[@class=\"title fw500 ellipsis\"]'):\n",
    "        Skills.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Skills.append(\"_\")\n",
    "# Scraping Location\n",
    "try:\n",
    "    for i in driver.find_elements(By.XPATH,'//*[@class=\"ellipsis fleft fs12 lh16 locWdth\"]'):\n",
    "        Location.append(i.text)\n",
    "except NoSuchElementException:\n",
    "    Location.append(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d1f0f454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 19 19 19 19\n"
     ]
    }
   ],
   "source": [
    "print(len(Name),len(Designation),len(Company),len(Skills),len(Location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b2b6d279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst - Data Science</td>\n",
       "      <td>Data Analyst - Data Science</td>\n",
       "      <td>Altigreen</td>\n",
       "      <td>Data Analyst - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sr Data Science Analyst</td>\n",
       "      <td>Sr Data Science Analyst</td>\n",
       "      <td>Transunion</td>\n",
       "      <td>Sr Data Science Analyst</td>\n",
       "      <td>Hybrid - Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Analyst, Data Science &amp; Analytics</td>\n",
       "      <td>Senior Analyst, Data Science &amp; Analytics</td>\n",
       "      <td>Transunion</td>\n",
       "      <td>Senior Analyst, Data Science &amp; Analytics</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Rapido</td>\n",
       "      <td>Data Science Manager</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Salesforce</td>\n",
       "      <td>Data Science Engineer</td>\n",
       "      <td>Hyderabad/Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Paypal</td>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>Varite</td>\n",
       "      <td>Senior Analyst, Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Analyst-Data Science</td>\n",
       "      <td>Senior Analyst-Data Science</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Senior Analyst-Data Science</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lead - Data Science / Predictive Modelling</td>\n",
       "      <td>Lead - Data Science / Predictive Modelling</td>\n",
       "      <td>Indiainfoline</td>\n",
       "      <td>Lead - Data Science / Predictive Modelling</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Storytelling / Visualization Role - Data ...</td>\n",
       "      <td>Data Storytelling / Visualization Role - Data ...</td>\n",
       "      <td>Indiainfoline</td>\n",
       "      <td>Data Storytelling / Visualization Role - Data ...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Manager Data Science</td>\n",
       "      <td>Manager Data Science</td>\n",
       "      <td>Visa</td>\n",
       "      <td>Manager Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Principal Engineer - Data Science</td>\n",
       "      <td>Principal Engineer - Data Science</td>\n",
       "      <td>GMT Infotech</td>\n",
       "      <td>Principal Engineer - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Science Engineer (Synthetic Data Generation)</td>\n",
       "      <td>Data Science Engineer (Synthetic Data Generation)</td>\n",
       "      <td>Calsoft</td>\n",
       "      <td>Data Science Engineer (Synthetic Data Generation)</td>\n",
       "      <td>Kolkata, Indore, Pune, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Xoom Inc</td>\n",
       "      <td>Sr. Manager Data Science</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ACN - Applied Intelligence - CC - Data Science...</td>\n",
       "      <td>ACN - Applied Intelligence - CC - Data Science...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>ACN - Applied Intelligence - CC - Data Science...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Science Analyst (From FMCG Ind.)and Sales...</td>\n",
       "      <td>Data Science Analyst (From FMCG Ind.)and Sales...</td>\n",
       "      <td>H K Jewels</td>\n",
       "      <td>Data Science Analyst (From FMCG Ind.)and Sales...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Senior Investment Risk Specialist - Data Science</td>\n",
       "      <td>Senior Investment Risk Specialist - Data Science</td>\n",
       "      <td>Wells Fargo</td>\n",
       "      <td>Senior Investment Risk Specialist - Data Science</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Hiring For Data Science role (NLP) - Max 30 Da...</td>\n",
       "      <td>Hiring For Data Science role (NLP) - Max 30 Da...</td>\n",
       "      <td>Genpact</td>\n",
       "      <td>Hiring For Data Science role (NLP) - Max 30 Da...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Data Science Consultant / Trainer</td>\n",
       "      <td>Data Science Consultant / Trainer</td>\n",
       "      <td>ACODS</td>\n",
       "      <td>Data Science Consultant / Trainer</td>\n",
       "      <td>Noida</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name  \\\n",
       "0                         Data Analyst - Data Science   \n",
       "1                             Sr Data Science Analyst   \n",
       "2            Senior Analyst, Data Science & Analytics   \n",
       "3                                Data Science Manager   \n",
       "4                               Data Science Engineer   \n",
       "5                            Sr. Manager Data Science   \n",
       "6                        Senior Analyst, Data Science   \n",
       "7                         Senior Analyst-Data Science   \n",
       "8          Lead - Data Science / Predictive Modelling   \n",
       "9   Data Storytelling / Visualization Role - Data ...   \n",
       "10                               Manager Data Science   \n",
       "11                  Principal Engineer - Data Science   \n",
       "12  Data Science Engineer (Synthetic Data Generation)   \n",
       "13                           Sr. Manager Data Science   \n",
       "14  ACN - Applied Intelligence - CC - Data Science...   \n",
       "15  Data Science Analyst (From FMCG Ind.)and Sales...   \n",
       "16   Senior Investment Risk Specialist - Data Science   \n",
       "17  Hiring For Data Science role (NLP) - Max 30 Da...   \n",
       "18                  Data Science Consultant / Trainer   \n",
       "\n",
       "                                          Designation        Company  \\\n",
       "0                         Data Analyst - Data Science      Altigreen   \n",
       "1                             Sr Data Science Analyst     Transunion   \n",
       "2            Senior Analyst, Data Science & Analytics     Transunion   \n",
       "3                                Data Science Manager         Rapido   \n",
       "4                               Data Science Engineer     Salesforce   \n",
       "5                            Sr. Manager Data Science         Paypal   \n",
       "6                        Senior Analyst, Data Science         Varite   \n",
       "7                         Senior Analyst-Data Science      Accenture   \n",
       "8          Lead - Data Science / Predictive Modelling  Indiainfoline   \n",
       "9   Data Storytelling / Visualization Role - Data ...  Indiainfoline   \n",
       "10                               Manager Data Science           Visa   \n",
       "11                  Principal Engineer - Data Science   GMT Infotech   \n",
       "12  Data Science Engineer (Synthetic Data Generation)        Calsoft   \n",
       "13                           Sr. Manager Data Science       Xoom Inc   \n",
       "14  ACN - Applied Intelligence - CC - Data Science...      Accenture   \n",
       "15  Data Science Analyst (From FMCG Ind.)and Sales...     H K Jewels   \n",
       "16   Senior Investment Risk Specialist - Data Science    Wells Fargo   \n",
       "17  Hiring For Data Science role (NLP) - Max 30 Da...        Genpact   \n",
       "18                  Data Science Consultant / Trainer          ACODS   \n",
       "\n",
       "                                               Skills  \\\n",
       "0                         Data Analyst - Data Science   \n",
       "1                             Sr Data Science Analyst   \n",
       "2            Senior Analyst, Data Science & Analytics   \n",
       "3                                Data Science Manager   \n",
       "4                               Data Science Engineer   \n",
       "5                            Sr. Manager Data Science   \n",
       "6                        Senior Analyst, Data Science   \n",
       "7                         Senior Analyst-Data Science   \n",
       "8          Lead - Data Science / Predictive Modelling   \n",
       "9   Data Storytelling / Visualization Role - Data ...   \n",
       "10                               Manager Data Science   \n",
       "11                  Principal Engineer - Data Science   \n",
       "12  Data Science Engineer (Synthetic Data Generation)   \n",
       "13                           Sr. Manager Data Science   \n",
       "14  ACN - Applied Intelligence - CC - Data Science...   \n",
       "15  Data Science Analyst (From FMCG Ind.)and Sales...   \n",
       "16   Senior Investment Risk Specialist - Data Science   \n",
       "17  Hiring For Data Science role (NLP) - Max 30 Da...   \n",
       "18                  Data Science Consultant / Trainer   \n",
       "\n",
       "                                      Location  \n",
       "0                          Bangalore/Bengaluru  \n",
       "1                             Hybrid - Chennai  \n",
       "2                                         Pune  \n",
       "3                          Bangalore/Bengaluru  \n",
       "4                       Hyderabad/Secunderabad  \n",
       "5                                      Chennai  \n",
       "6                          Bangalore/Bengaluru  \n",
       "7                                       Mumbai  \n",
       "8                          Bangalore/Bengaluru  \n",
       "9                          Bangalore/Bengaluru  \n",
       "10                         Bangalore/Bengaluru  \n",
       "11                         Bangalore/Bengaluru  \n",
       "12  Kolkata, Indore, Pune, Bangalore/Bengaluru  \n",
       "13                                     Chennai  \n",
       "14                            Gurgaon/Gurugram  \n",
       "15                                      Mumbai  \n",
       "16                         Bangalore/Bengaluru  \n",
       "17                            Gurgaon/Gurugram  \n",
       "18                                       Noida  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame()\n",
    "df['Name']=Name\n",
    "df['Designation']=Designation\n",
    "df['Company']=Company\n",
    "df['Skills']=Skills\n",
    "df['Location']=Location\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "596c0e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"E:\\Flip Robo\\Data\\Job.csv\", index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2677a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ddf1a5",
   "metadata": {},
   "source": [
    "Q8. Scrape the details of Highest selling novels.  \n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/  \n",
    "You have to find the following details:  \n",
    "A) Book name  \n",
    "B) Author name  \n",
    "C) Volumes sold  \n",
    "D) Publisher  \n",
    "E) Genre \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9bece9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to webdriver\n",
    "driver = webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "\n",
    "# get webpage\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b67423bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Bookname = []\n",
    "Authorname = []\n",
    "Volumessold = []\n",
    "Publisher = []\n",
    "Genre = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "57d8b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping book names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody//tr/td[2]\"):\n",
    "    Bookname.append(i.text)\n",
    "    \n",
    "#Scraping author names data\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Authorname.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Authorname.append('-')\n",
    "time.sleep(1)\n",
    "\n",
    "#Scraping data of volumes sold\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[4]\"):\n",
    "    Volumessold.append(i.text)\n",
    "    \n",
    "#Scraping data of publisher names\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//tbody/tr/td[6]\"):\n",
    "    Genre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc98d3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volume sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book Name            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating dataframe for scraped data\n",
    "Novels=pd.DataFrame({})\n",
    "Novels['Book Name'] = Bookname\n",
    "Novels['Author'] = Authorname\n",
    "Novels['Volume sold'] = Volumessold\n",
    "Novels['Publisher'] = Publisher\n",
    "Novels['Genre'] = Genre\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f823109",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3d677",
   "metadata": {},
   "source": [
    "Q9. Scrape the details most watched tv series of all time from imdb.com.  \n",
    "Url = https://www.imdb.com/list/ls095964455/  \n",
    "You have to find the following details:  \n",
    "A) Name  \n",
    "B) Year span  \n",
    "C) Genre  \n",
    "D) Run time  \n",
    "E) Ratings  \n",
    "F) Votes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d00446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to webdriver\n",
    "driver = webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "# get webpage\n",
    "driver.get(\"https://www.imdb.com/list/ls095964455/\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "98f86c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists.\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []\n",
    "\n",
    "#Scraping data of Names\n",
    "for i in driver.find_elements(By.XPATH,\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping data of Year span\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "    \n",
    "#Scraping data of Run time\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "#Scraping data of Ratings\n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements(By.XPATH,\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "#Scraping data of votes\n",
    "for i in driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8b0fb2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year Span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011‚Äì2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,064,283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016‚Äì )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,157,430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010‚Äì2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>973,585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017‚Äì2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>290,217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014‚Äì2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>249,521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reign</td>\n",
       "      <td>(2013‚Äì2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>49,732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>A Series of Unfortunate Events</td>\n",
       "      <td>(2017‚Äì2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>60,796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Criminal Minds</td>\n",
       "      <td>(2005‚Äì2020)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>195,784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Scream: The TV Series</td>\n",
       "      <td>(2015‚Äì2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7.1</td>\n",
       "      <td>41,191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>The Haunting of Hill House</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>238,805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name    Year Span                     Genre  \\\n",
       "0                  Game of Thrones  (2011‚Äì2019)  Action, Adventure, Drama   \n",
       "1                  Stranger Things     (2016‚Äì )    Drama, Fantasy, Horror   \n",
       "2                 The Walking Dead  (2010‚Äì2022)   Drama, Horror, Thriller   \n",
       "3                   13 Reasons Why  (2017‚Äì2020)  Drama, Mystery, Thriller   \n",
       "4                          The 100  (2014‚Äì2020)    Drama, Mystery, Sci-Fi   \n",
       "..                             ...          ...                       ...   \n",
       "95                           Reign  (2013‚Äì2017)                     Drama   \n",
       "96  A Series of Unfortunate Events  (2017‚Äì2019)  Adventure, Comedy, Drama   \n",
       "97                  Criminal Minds  (2005‚Äì2020)     Crime, Drama, Mystery   \n",
       "98           Scream: The TV Series  (2015‚Äì2019)      Comedy, Crime, Drama   \n",
       "99      The Haunting of Hill House       (2018)    Drama, Horror, Mystery   \n",
       "\n",
       "   Run Time Ratings      Votes  \n",
       "0    57 min     9.2  2,064,283  \n",
       "1    51 min     8.7  1,157,430  \n",
       "2    44 min     8.1    973,585  \n",
       "3    60 min     7.5    290,217  \n",
       "4    43 min     7.6    249,521  \n",
       "..      ...     ...        ...  \n",
       "95   42 min     7.4     49,732  \n",
       "96   50 min     7.8     60,796  \n",
       "97   42 min     8.1    195,784  \n",
       "98   45 min     7.1     41,191  \n",
       "99  572 min     8.6    238,805  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataframe for scraped data\n",
    "TVseries=pd.DataFrame({})\n",
    "TVseries['Name'] = Name\n",
    "TVseries['Year Span'] = Year_span\n",
    "TVseries['Genre'] = Genre\n",
    "TVseries['Run Time'] = Run_time\n",
    "TVseries['Ratings'] = Ratings\n",
    "TVseries['Votes'] = Votes\n",
    "TVseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d8f2a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00df65",
   "metadata": {},
   "source": [
    "Q10. Details of Datasets from UCI machine learning repositories.  \n",
    "Url = https://archive.ics.uci.edu/  \n",
    "You have to find the following details:  \n",
    "A) Dataset name  \n",
    "B) Data type  \n",
    "C) Task  \n",
    "D) Attribute type  \n",
    "E) No of instances  \n",
    "F) No of attribute  \n",
    "G) Year  \n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cf22f68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to webdriver\n",
    "driver = webdriver.Chrome(r\"E:\\Flip Robo\\chromedriver.exe\")\n",
    "# get webpage\n",
    "driver.get(\" https://archive.ics.uci.edu/\")\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a382fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding view all dataset button from the webpage\n",
    "viewall_dataset = driver.find_element(By.XPATH,\"//tbody[1]//tr/td[2]/span[2]/a\")    \n",
    "page_url = viewall_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2dffffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching page urls of all datasets \n",
    "view_list = driver.find_element(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a\")  \n",
    "list_url = view_list.get_attribute(\"href\")           \n",
    "driver.get(list_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1b4f17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching urls for each dataset\n",
    "dataset_url = driver.find_elements(By.XPATH,\"//p[@class='normal']//b/a\")    \n",
    "\n",
    "urls = []     \n",
    "for i in dataset_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8c0f3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists\n",
    "Dataset_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attribute_type = []\n",
    "No_of_instances = []\n",
    "No_of_attributes = []\n",
    "Year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc531ff6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=106.0.5249.103)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00241ED3+2236115]\n\tOrdinal0 [0x001D92F1+1807089]\n\tOrdinal0 [0x000E66FD+812797]\n\tOrdinal0 [0x000CDFFA+712698]\n\tOrdinal0 [0x0013506B+1134699]\n\tOrdinal0 [0x0014514A+1200458]\n\tOrdinal0 [0x001318A6+1120422]\n\tOrdinal0 [0x0010A73D+960317]\n\tOrdinal0 [0x0010B71F+964383]\n\tGetHandleVerifier [0x004EE7E2+2743074]\n\tGetHandleVerifier [0x004E08D4+2685972]\n\tGetHandleVerifier [0x002D2BAA+532202]\n\tGetHandleVerifier [0x002D1990+527568]\n\tOrdinal0 [0x001E080C+1837068]\n\tOrdinal0 [0x001E4CD8+1854680]\n\tOrdinal0 [0x001E4DC5+1854917]\n\tOrdinal0 [0x001EED64+1895780]\n\tBaseThreadInitThunk [0x7630FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77557B5E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77557B2E+238]\n\t(No symbol) [0x00000000]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchWindowException\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [129]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Scraping Dataset name\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m----> 7\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//span[@class=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheading\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     Dataset_name\u001b[38;5;241m.\u001b[39mappend(dataset_name\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchElementException:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:855\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    852\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[0;32m    853\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m value\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:428\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    426\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 428\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m     response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(\n\u001b[0;32m    430\u001b[0m         response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:243\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mNoSuchWindowException\u001b[0m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=106.0.5249.103)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x00241ED3+2236115]\n\tOrdinal0 [0x001D92F1+1807089]\n\tOrdinal0 [0x000E66FD+812797]\n\tOrdinal0 [0x000CDFFA+712698]\n\tOrdinal0 [0x0013506B+1134699]\n\tOrdinal0 [0x0014514A+1200458]\n\tOrdinal0 [0x001318A6+1120422]\n\tOrdinal0 [0x0010A73D+960317]\n\tOrdinal0 [0x0010B71F+964383]\n\tGetHandleVerifier [0x004EE7E2+2743074]\n\tGetHandleVerifier [0x004E08D4+2685972]\n\tGetHandleVerifier [0x002D2BAA+532202]\n\tGetHandleVerifier [0x002D1990+527568]\n\tOrdinal0 [0x001E080C+1837068]\n\tOrdinal0 [0x001E4CD8+1854680]\n\tOrdinal0 [0x001E4DC5+1854917]\n\tOrdinal0 [0x001EED64+1895780]\n\tBaseThreadInitThunk [0x7630FA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77557B5E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77557B2E+238]\n\t(No symbol) [0x00000000]\n"
     ]
    }
   ],
   "source": [
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    #Scraping Dataset name\n",
    "    try: \n",
    "        dataset_name = driver.find_element(By.XPATH,\"//span[@class='heading']\")\n",
    "        Dataset_name.append(dataset_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Dataset_name.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping data type\n",
    "    try:\n",
    "        data_type = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr/td[2]\")\n",
    "        if data_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(data_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    \n",
    "    #scraping Task\n",
    "    try:\n",
    "        task = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[3]/td[2]\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    \n",
    "    \n",
    "    # Scraping Attribute type\n",
    "    try:\n",
    "        attribute_type = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[2]/td[2]\")\n",
    "        if attribute_type.text == \"N/A\": raise NoSuchElementException\n",
    "        Attribute_type.append(attribute_type.text)\n",
    "    except NoSuchElementException:\n",
    "        Attribute_type.append('-')\n",
    "    \n",
    "    \n",
    "    # Scraping No of instances\n",
    "    try:\n",
    "        instances = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if instances.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_instances.append(instances.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_instances.append('-')\n",
    "    \n",
    "    \n",
    "    # Scraping No of attributes\n",
    "    try:\n",
    "        attribute = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attribute.text == \"N/A\": raise NoSuchElementException\n",
    "        No_of_attributes.append(attribute.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_attributes.append('-')\n",
    "        \n",
    "      \n",
    "    # Scraping year\n",
    "    try:\n",
    "        year = driver.find_element(By.XPATH,\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ae713ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Name</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute type</th>\n",
       "      <th>No of instance</th>\n",
       "      <th>No of attributes</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.4 GHZ Indoor Channel Measurements Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>7840</td>\n",
       "      <td>5</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3D Road Network (North Jutland, Denmark) Data Set</td>\n",
       "      <td>Sequential, Text</td>\n",
       "      <td>Regression, Clustering</td>\n",
       "      <td>Real</td>\n",
       "      <td>434874</td>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3W dataset Data Set</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>1984</td>\n",
       "      <td>8</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9mers from cullpdb Data Set</td>\n",
       "      <td>Sequential</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>158716</td>\n",
       "      <td>4</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>: Simulated Data set of Iraqi tourism places D...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Clustering</td>\n",
       "      <td>-</td>\n",
       "      <td>232</td>\n",
       "      <td>16</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Parkinson Speech Dataset with Multiple Types o...</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>1040</td>\n",
       "      <td>26</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Parkinson's Disease Classification Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>756</td>\n",
       "      <td>754</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Parkinsons Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>197</td>\n",
       "      <td>23</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Parkinsons Telemonitoring Data Set</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Integer, Real</td>\n",
       "      <td>5875</td>\n",
       "      <td>26</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Pedal Me Bicycle Deliveries Data Set</td>\n",
       "      <td>Time-Series</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Data Name  \\\n",
       "0         2.4 GHZ Indoor Channel Measurements Data Set   \n",
       "1    3D Road Network (North Jutland, Denmark) Data Set   \n",
       "2                                  3W dataset Data Set   \n",
       "3                          9mers from cullpdb Data Set   \n",
       "4    : Simulated Data set of Iraqi tourism places D...   \n",
       "..                                                 ...   \n",
       "402  Parkinson Speech Dataset with Multiple Types o...   \n",
       "403        Parkinson's Disease Classification Data Set   \n",
       "404                                Parkinsons Data Set   \n",
       "405                 Parkinsons Telemonitoring Data Set   \n",
       "406               Pedal Me Bicycle Deliveries Data Set   \n",
       "\n",
       "                     Data Type                        Task Attribute type  \\\n",
       "0                 Multivariate              Classification           Real   \n",
       "1             Sequential, Text      Regression, Clustering           Real   \n",
       "2    Multivariate, Time-Series  Classification, Clustering  Integer, Real   \n",
       "3                   Sequential  Classification, Regression           Real   \n",
       "4                 Multivariate  Classification, Clustering              -   \n",
       "..                         ...                         ...            ...   \n",
       "402               Multivariate  Classification, Regression  Integer, Real   \n",
       "403               Multivariate              Classification  Integer, Real   \n",
       "404               Multivariate              Classification           Real   \n",
       "405               Multivariate                  Regression  Integer, Real   \n",
       "406                Time-Series                  Regression           Real   \n",
       "\n",
       "    No of instance No of attributes  Year  \n",
       "0             7840                5  2018  \n",
       "1           434874                4  2013  \n",
       "2             1984                8  2019  \n",
       "3           158716                4  2021  \n",
       "4              232               16  2020  \n",
       "..             ...              ...   ...  \n",
       "402           1040               26  2014  \n",
       "403            756              754  2018  \n",
       "404            197               23  2008  \n",
       "405           5875               26  2009  \n",
       "406             36               15  2021  \n",
       "\n",
       "[407 rows x 7 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating dataframe \n",
    "\n",
    "ML=pd.DataFrame({})\n",
    "ML['Data Name'] = Dataset_name[:622]\n",
    "ML['Data Type'] = Data_type[:622]\n",
    "ML['Task'] = Task[:622]\n",
    "ML['Attribute type'] = Attribute_type[:622]\n",
    "ML['No of instance'] = No_of_instances[:622]\n",
    "ML['No of attributes'] = No_of_attributes[:622]\n",
    "ML['Year'] = Year[:622]\n",
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef93ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d217c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
